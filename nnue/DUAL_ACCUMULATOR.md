# Dual-Accumulator NNUE (Stockfish-Style)

## Summary

Your NNUE now uses **dual-accumulator architecture**, which is the state-of-the-art approach used by Stockfish and other top engines. This gives you the best of both worlds:

âœ… **Side-to-move perspective encoding** (better learning)  
âœ… **Incremental updates still work** (fast inference)  
âœ… **No redundant information** (768 features, no castling/STM bits)

## What Changed

### Before (Single Accumulator + STM Bit)
- 773 features (768 pieces + 1 STM + 4 castling)
- Single embedding accumulator
- Network had to learn from tiny STM bit signal
- Incremental updates worked but encoding was suboptimal

### After (Dual Accumulator)
- **768 features per perspective** (just pieces)
- **Two embedding accumulators** (white perspective + black perspective)
- Both accumulators maintained incrementally
- Select correct accumulator at inference time
- Network learns one evaluation function from "my perspective"

## Architecture (Stockfish-Style)

```
Position â†’ Encode both perspectives
           â†“
        [White Bitset]  [Black Bitset]
           â†“                â†“
    [White Accumulator] [Black Accumulator]
         256 dims           256 dims
           â†“                â†“
     Incremental       Incremental
     Updates           Updates
           â†“                â†“
        Concatenate [STM, Opponent]
           â†“
        [512 dims] (256 + 256)
           â†“
     [ReLU â†’ Hidden1(512â†’32) â†’ Hidden2(32â†’32) â†’ Output]
           â†“
      Evaluation (from STM perspective)
           â†“
      Negate if black to move
           â†“
      Final eval (white's perspective)
```

## Key Features

### 1. Dual Perspective Encoding

**White's perspective:**
- Our pieces: white pieces (indices 0-5)
- Their pieces: black pieces (indices 6-11)
- Squares: normal orientation

**Black's perspective:**
- Our pieces: black pieces (indices 0-5)  
- Their pieces: white pieces (indices 6-11)
- Squares: vertically flipped (a1â†”a8, etc.)

### 2. Incremental Updates Preserved

After each move:
- **Both accumulators are updated** (only changed features)
- White accumulator tracks from white's view
- Black accumulator tracks from black's view
- No full recomputation needed!

This is the key insight: even though we switch perspectives between moves, we maintain BOTH perspectives at all times, so no features need to be flipped.

### 3. Inference

```rust
fn evaluate(&mut self, board: &Board) -> i16 {
    // Encode from both perspectives
    let white_bitset = encode_board_bitset_perspective(board, Color::White);
    let black_bitset = encode_board_bitset_perspective(board, Color::Black);
    
    // Update both accumulators incrementally
    // Select side-to-move's accumulator
    let use_white = board.side_to_move() == Color::White;
    let eval = self.nnue_network.forward_dual(
        &white_bitset,
        &black_bitset, 
        use_white
    );
    
    // Negate if black to move (get white's perspective)
    if !use_white { -eval } else { eval }
}
```

## Performance

**Memory:**
- 2Ã— embedding buffers (2 Ã— 256 Ã— 4 bytes = 2KB)
- Shared hidden layer buffers
- Total overhead: ~2KB extra (smaller than before!)

**Compute:**
- Both accumulators updated incrementally (2Ã— update cost)
- But updates are sparse (only changed features)
- Hidden layers see 512 dims (concatenated 256+256)
- Overall: ~2Ã— cost for embedding updates, rest unchanged

**Benefits of 256 per accumulator:**
- Smaller model size (less memory, faster to load)
- More efficient cache usage
- Network sees BOTH perspectives (more information!)
- This is exactly what Stockfish does

**In practice:** The 2Ã— embedding update cost is negligible compared to search, and you get much better evaluation quality because the network sees both king perspectives simultaneously.

## Why This is Better

### Compared to Single Accumulator + STM Bit:
âœ… Better learning (network doesn't fight tiny STM bit signal)  
âœ… More principled encoding  
âœ… Proven architecture (Stockfish uses this)

### Compared to Naive STM Perspective:
âœ… Incremental updates still work (huge speed gain)  
âœ… No feature flipping on every move

## Training Implications

When you retrain your network:

1. **Data generation:** Generate positions with both perspectives
2. **Training samples:** Each position generates ONE sample from the perspective of the side to move
3. **Network learns:** "This position is good/bad for me (the side to move)"
4. **No bias:** Network doesn't learn "good for white" vs "good for black" separately

The network becomes color-agnostic and learns pure positional evaluation.

## Next Steps

1. **Retrain your network** with the new encoding:
   ```bash
   cargo run --release --bin generate -- --output training_data.bin
   cargo run --release --bin train -- --input training_data.bin
   ```

2. **Consider architecture adjustments:**
   - The 768 features are more expressive than your old 773
   - You might get away with the same embedding size (512)
   - Or increase to 768-1024 for more capacity

3. **Expect better results:**
   - Better positional understanding
   - More accurate tactical evaluation
   - Improved endgame play
   - Color-symmetric evaluation

## Implementation Details

**Files modified:**
- `nnue/src/encoding.rs` - Dual perspective encoding functions
- `nnue/src/network.rs` - Dual accumulators, incremental update logic
- `nnue/src/evaluator.rs` - Select correct accumulator at inference

**Key functions:**
- `encode_board_bitset_perspective(board, perspective)` - Encode from specific color's view
- `forward_dual(white_bitset, black_bitset, use_white)` - Dual accumulator forward pass
- Board flipping: `flip_square_vertical()` mirrors ranks when encoding black's perspective

## Further Reading

- [Stockfish NNUE Documentation](https://github.com/official-stockfish/nnue-pytorch)
- Search for "efficiently updatable neural network" papers
- The original NNUE paper by Yu Nasu

## Conclusion

You now have state-of-the-art NNUE architecture! This dual-accumulator approach:
- Learns better (proper side-to-move perspective)
- Runs fast (incremental updates preserved)
- Uses less redundant information (no STM bit needed)

Train it up and enjoy the stronger evaluation! ðŸš€

